# Guia de Execu√ß√£o do Projeto

Este guia explica como executar o projeto completo, desde a infraestrutura at√© o frontend.

## üìã Pr√©-requisitos

- Docker e Docker Compose instalados
- Vari√°veis de ambiente configuradas (crie um arquivo `.env` na raiz se necess√°rio)

## üöÄ Execu√ß√£o R√°pida (Tudo de uma vez)

```bash
make start-full
```

Este comando ir√°:
1. Construir todas as imagens
2. Subir a infraestrutura (Kafka, Postgres, Elasticsearch, MongoDB, Redis)
3. Iniciar os servi√ßos de coleta de dados (data_collection-api, worker)
4. Iniciar o pipeline (parser, classifier, db_sync)
5. Produzir uma mensagem de exemplo
6. Iniciar os servi√ßos backend (indexer, searcher)
7. Iniciar os servi√ßos de aplica√ß√£o (mock-api, backend-graphql, frontend)

## üìù Execu√ß√£o Passo a Passo

### 1. Subir a Infraestrutura

```bash
make run-infra
```

Ou manualmente:
```bash
docker compose up -d zookeeper kafka kafka-web postgres pgadmin elasticsearch kibana mongo data_collection_redis
```

**Aguarde ~15-20 segundos** para todos os servi√ßos ficarem prontos.

### 2. Verificar se o Kafka est√° funcionando

Acesse o Kafka Web UI: http://localhost:8081

### 3. Iniciar os Servi√ßos de Coleta de Dados

```bash
make run-data-collection-services
```

Ou manualmente:
```bash
docker compose up -d data_collection-api data_collection_worker
```

Isso iniciar√°:
- **data_collection-api**: API para receber requisi√ß√µes de coleta de processos
- **data_collection_worker**: Worker Celery que processa as coletas e publica no Kafka

**Aguarde alguns segundos** para os servi√ßos ficarem prontos.

### 4. Iniciar o Pipeline

```bash
make run-pipeline-apps
```

Ou manualmente:
```bash
docker compose up -d parser classifier db_sync
```

Isso iniciar√°:
- **parser**: Consome `lawsuit_raw` e publica em `lawsuit_structured`
- **classifier**: Consome `lawsuit_structured` e publica em `lawsuit_classified`
- **db_sync**: Consome `lawsuit_classified` e salva no PostgreSQL

### 5. Produzir uma Mensagem de Exemplo

```bash
make produce-example-message
```

Ou manualmente:
```bash
docker compose up initial-payload-producer
```

Isso ir√°:
- Ler o arquivo `initial_payload_producer/lawsuit.json`
- Publicar no t√≥pico `lawsuit_raw`
- O pipeline processar√° automaticamente: `raw ‚Üí structured ‚Üí classified`

### 6. Iniciar os Servi√ßos Backend

```bash
make run-backend-services
```

Ou manualmente:
```bash
docker compose up -d indexer-api searcher-api
```

Isso iniciar√°:
- **indexer-api**: Consome `lawsuit_classified` e indexa no Elasticsearch
- **searcher-api**: API de busca no Elasticsearch

**Aguarde alguns segundos** para o indexer processar a mensagem.

### 7. Iniciar os Servi√ßos de Aplica√ß√£o

```bash
make run-app-services
```

Ou manualmente:
```bash
docker compose up -d mock-api backend-graphql frontend
```

## üß™ Testar o Fluxo Completo

### 1. Verificar se o processo foi indexado

Acesse o Kibana: http://localhost:5601

Ou verifique diretamente no Elasticsearch:
```bash
curl http://localhost:9200/lawsuits/_search?pretty
```

### 2. Buscar no Frontend (Processo j√° indexado)

Acesse: http://localhost:3000

Busque pelo n√∫mero do processo: `1277567-49.2023.8.09.0001`

Este processo deve ser encontrado imediatamente se foi indexado pelo pipeline.

### 3. Testar Coleta Autom√°tica (Processo n√£o indexado)

Para testar o fluxo completo de coleta:

1. **Busque por um CNJ que n√£o existe no Elasticsearch** (ex: `0710802-55.2018.8.02.0001`)

2. **O sistema ir√°:**
   - Verificar no searcher (n√£o encontrar√°)
   - Chamar a API de coleta automaticamente
   - Exibir mensagem de "Processo em coleta" no frontend

3. **Aguarde alguns minutos** enquanto:
   - O worker coleta o processo
   - Publica no Kafka (`lawsuit_raw`)
   - O pipeline processa: `raw ‚Üí structured ‚Üí classified`
   - O indexer indexa no Elasticsearch

4. **Tente buscar novamente** - o processo deve aparecer agora!

### 4. Verificar Logs da Coleta

```bash
# Logs da API de coleta
make logs-data-collection

# Ou individualmente
docker compose logs -f data_collection-api
docker compose logs -f data_collection_worker
```

### 5. Verificar os Logs

```bash
# Logs do indexer (para ver se processou do Kafka)
make logs-indexer

# Logs do pipeline
make logs-pipeline

# Logs da coleta de dados
make logs-data-collection

# Todos os logs
make logs
```

## üîç Verificar o Fluxo no Kafka

1. Acesse http://localhost:8081
2. Verifique os t√≥picos:
   - `lawsuit_raw`: Mensagem inicial
   - `lawsuit_structured`: Mensagem processada pelo parser
   - `lawsuit_classified`: Mensagem classificada

## üõ†Ô∏è Comandos √öteis

```bash
# Parar tudo
make stop-all

# Ver logs de um servi√ßo espec√≠fico
docker compose logs -f indexer-api

# Reconstruir um servi√ßo espec√≠fico
docker compose build indexer-api
docker compose up -d indexer-api

# Ver status de todos os servi√ßos
docker compose ps

# Limpar tudo (volumes inclu√≠dos)
docker compose down -v
```

## üîÑ Fluxo Completo de Dados

### Fluxo 1: Pipeline Inicial (Mensagem de Exemplo)
```
1. initial_payload_producer
   ‚Üì (publica em lawsuit_raw)
2. parser
   ‚Üì (publica em lawsuit_structured)
3. classifier
   ‚Üì (publica em lawsuit_classified)
4. db_sync
   ‚Üì (salva no PostgreSQL)
5. indexer-api (consome lawsuit_classified)
   ‚Üì (indexa no Elasticsearch)
6. searcher-api
   ‚Üì (busca no Elasticsearch)
7. backend-graphql
   ‚Üì (exp√µe GraphQL)
8. frontend
   ‚Üì (consome GraphQL)
```

### Fluxo 2: Coleta Autom√°tica (Processo n√£o encontrado)
```
1. Usu√°rio busca CNJ no frontend
   ‚Üì
2. backend-graphql chama searcher-api
   ‚Üì (n√£o encontra - hits: 0)
3. backend-graphql chama data_collection-api
   ‚Üì
4. data_collection-api verifica cache (MongoDB)
   ‚Üì (n√£o encontra)
5. data_collection-api enfileira tarefa no Redis
   ‚Üì
6. data_collection_worker processa coleta
   ‚Üì (coleta dados do tribunal)
7. data_collection_worker salva no MongoDB
   ‚Üì
8. data_collection_worker publica no Kafka (lawsuit_raw)
   ‚Üì
9. parser processa
   ‚Üì (publica em lawsuit_structured)
10. classifier processa
   ‚Üì (publica em lawsuit_classified)
11. indexer-api indexa no Elasticsearch
   ‚Üì
12. Pr√≥xima busca: processo encontrado! ‚úÖ
```

## ‚ö†Ô∏è Troubleshooting

### Kafka n√£o est√° recebendo mensagens
- Verifique se o zookeeper e kafka est√£o rodando: `docker compose ps`
- Verifique os logs: `docker compose logs kafka`

### Indexer n√£o est√° processando
- Verifique se o Kafka est√° acess√≠vel: `docker compose logs indexer-api`
- Verifique se h√° mensagens no t√≥pico `lawsuit_classified` no Kafka Web UI

### Frontend n√£o encontra processos
- Verifique se o Elasticsearch tem dados: `curl http://localhost:9200/lawsuits/_count`
- Verifique os logs do searcher: `docker compose logs searcher-api`
- Verifique os logs do backend-graphql: `docker compose logs backend-graphql`

### Coleta de dados n√£o est√° funcionando
- Verifique se MongoDB e Redis est√£o rodando: `docker compose ps mongo data_collection_redis`
- Verifique os logs da API: `docker compose logs data_collection-api`
- Verifique os logs do worker: `docker compose logs data_collection_worker`
- Verifique se o Kafka est√° acess√≠vel: `docker compose logs kafka`
- Teste a API diretamente: `curl "http://localhost:8200/lawsuit?lawsuit_number=0710802-55.2018.8.02.0001"`

## üìä Portas dos Servi√ßos

- **Frontend**: http://localhost:3000
- **Backend GraphQL**: http://localhost:4000
- **Data Collection API**: http://localhost:8200
- **Searcher API**: http://localhost:8100
- **Indexer API**: http://localhost:8000
- **Mock API**: http://localhost:9777
- **Kafka Web UI**: http://localhost:8081
- **Kibana**: http://localhost:5601
- **Elasticsearch**: http://localhost:9200
- **PostgreSQL**: localhost:5432
- **PgAdmin**: http://localhost:8080
- **MongoDB**: localhost:27017
- **Redis**: localhost:6379

